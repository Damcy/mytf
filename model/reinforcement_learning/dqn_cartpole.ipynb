{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Damcy/.pyenv/versions/3.6.1/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "ENV_NAME = \"CartPole-v0\"\n",
    "EPISODE = 10000\n",
    "STEP = 1000\n",
    "TEST_TIME = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INITIAL_EPSILON = 0.99\n",
    "FINAL_EPSILON = 0.01\n",
    "REPLAY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNIT = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.9\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        # init experience replay\n",
    "        self.replay_buff = deque()\n",
    "        # parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.hidden_layer_unit = HIDDEN_UNIT\n",
    "        self.lr = LEARNING_RATE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.gamma = GAMMA\n",
    "        \n",
    "        self.create_Q_network()\n",
    "        self.create_training_method()\n",
    "        \n",
    "        # init session\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def create_Q_network(self):\n",
    "        # network\n",
    "        self.state_input = tf.placeholder(\"float32\", [None, self.state_dim])\n",
    "        hidden_layer = tf.contrib.layers.fully_connected(\n",
    "            inputs=self.state_input,\n",
    "            weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "            biases_initializer=tf.constant_initializer(0.01),\n",
    "            num_outputs=self.hidden_layer_unit,\n",
    "            scope='hidden_layer', activation_fn=tf.nn.relu\n",
    "        )\n",
    "        self.Q_value = tf.contrib.layers.fully_connected(\n",
    "            inputs=hidden_layer,\n",
    "            weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "            biases_initializer=tf.constant_initializer(0.01),\n",
    "            num_outputs=self.action_dim,\n",
    "            scope='output_layer', activation_fn=None\n",
    "        )\n",
    "    \n",
    "    def create_training_method(self):\n",
    "        self.action_input = tf.placeholder(\"float32\", [None, self.action_dim])\n",
    "        self.y_input = tf.placeholder(\"float32\", [None])\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Q_value, self.action_input), reduction_indices=1)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.cost)\n",
    "    \n",
    "    def perceive(self, state, action, reward, next_state, done):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buff.append((state, one_hot_action, reward, next_state, done))\n",
    "        if len(self.replay_buff) > REPLAY_SIZE:\n",
    "            self.replay_buff.popleft()\n",
    "        if len(self.replay_buff) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "    \n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "        # get mini-batch\n",
    "        mini_batch = random.sample(self.replay_buff, self.batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        action_batch = [data[1] for data in mini_batch]\n",
    "        reward_batch = [data[2] for data in mini_batch]\n",
    "        next_state_batch = [data[3] for data in mini_batch]\n",
    "        \n",
    "        #\n",
    "        y_batch = []\n",
    "        next_state_Q_value_batch = self.Q_value.eval(feed_dict={self.state_input: next_state_batch})\n",
    "        for i in range(0, self.batch_size):\n",
    "            done = mini_batch[i][4]\n",
    "            if done:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else:\n",
    "                y_batch.append(reward_batch[i] + self.gamma * np.max(next_state_Q_value_batch[i]))\n",
    "        \n",
    "        # training\n",
    "        self.optimizer.run(feed_dict={\n",
    "            self.y_input: y_batch,\n",
    "            self.action_input: action_batch,\n",
    "            self.state_input: state_batch\n",
    "        })\n",
    "    \n",
    "    def egreedy_action(self, state):\n",
    "        Q_value = self.Q_value.eval(feed_dict={\n",
    "            self.state_input: [state]\n",
    "        })[0]\n",
    "        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / (EPISODE * STEP * 1.0)\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            return np.argmax(Q_value)\n",
    "    \n",
    "    def action(self, state):\n",
    "        return np.argmax(self.Q_value.eval(feed_dict={\n",
    "            self.state_input: [state]\n",
    "        })[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    agent = DQN(env)\n",
    "    \n",
    "    for i in range(2):\n",
    "        env.reset()\n",
    "        for _ in range(100):\n",
    "            env.render()\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "    for episode in range(0, EPISODE):\n",
    "        # testing\n",
    "        if episode % 100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(0, TEST_TIME):\n",
    "                state = env.reset()\n",
    "                for step in range(0, STEP):\n",
    "                    env.render()\n",
    "                    action = agent.action(state)\n",
    "                    state, reward, done, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "            avg_reward = total_reward / TEST_TIME\n",
    "            print('episode: ', episode, 'Evaluation Average Reward:', avg_reward)\n",
    "            if avg_reward >= 200:\n",
    "                break\n",
    "\n",
    "        state = env.reset()\n",
    "        # training\n",
    "        for step in range(0, STEP):\n",
    "            action = agent.egreedy_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_agent = -1 if done else 0.1\n",
    "            agent.perceive(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n",
      "You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Evaluation Average Reward: 9.28\n",
      "episode:  100 Evaluation Average Reward: 9.36\n",
      "episode:  200 Evaluation Average Reward: 9.48\n",
      "episode:  300 Evaluation Average Reward: 9.37\n",
      "episode:  400 Evaluation Average Reward: 9.35\n",
      "episode:  500 Evaluation Average Reward: 9.4\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
